\include{settings}

\begin{document}

\include{title}

\tableofcontents
\newpage

\section{Программа работы}

\begin{enumerate}
	\item Изучить алгоритм кластеризации \code{DBSCAN}.
	\item Выделить параметры алгоритма.
	\item Написать программу кластеризации на базе заданного алгоритма.
	\item Проверить работоспособность на тестовых примерах базы \cite{data}. Привести результаты работы программы (таблицы и графики) при разных значениях параметров алгоритма для всех тестовых данных.
	\item Описать достоинства и недостатки алгоритма, а также способы модификаций и развития для улучшения качества работы.
	\item Привести альтернативные способы решения задачи кластеризации.
\end{enumerate}

\section{Выполнение работы}

\subsection{Алгоритм кластеризации \code{DBSCAN}}

Основанная на плотности пространственная кластеризация для приложений с шумами (англ. Density-based spatial clustering of applications with noise, DBSCAN) -- это алгоритм кластеризации данных, который предложили Маритин Эстер, Ганс-Петер Кригель, Ёрг Сандер и Сяовэй Су в 1996 \cite{dbscan}. Это алгоритм кластеризации, основанной на плотности -- если дан набор точек в некотором пространстве, алгоритм группирует вместе точки, которые тесно расположены (точки со многими близкими соседями), помечая как выбросы точки, которые находятся одиноко в областях с малой плотностью (ближайшие соседи которых лежат далеко). \code{DBSCAN} является одним из наиболее часто используемых алгоритмов кластеризации, и наиболее часто упоминается в научной литературе \cite{wiki}.

Приведем формальное описание алгоритма \cite{wiki,habr}. Введём несколько определений. Пусть задана некоторая симметричная функция расстояния $\rho(x, y)$ и константы $\epsilon$ и $m$.

\begin{enumerate}
	\item Назовём область $E(x)$, для которой $\forall y: \rho(x, y) \leq \epsilon$, $\epsilon$-окрестностью объекта $x$.
	\item Корневым объектом или ядерным объектом степени $m$ называется объект, $\epsilon$-окрестность которого содержит не менее $m$ объектов: $\left| E(x) \right| \geq m$.
	\item Объект $p$ непосредственно плотно-достижим из объекта $q$, если $p \in E(q)$ и $q$ -- корневой объект.
	\item Объект $p$ плотно-достижим из объекта $q$, если $\exists p_1, \dots, p_n, p_1 = q, p_n = p$, такие что $\forall i \in 1 \dots n - 1: p_{i + 1}$ непосредственно плотно-достижим из $p_i$.
	\item Все точки, не достижимые из основных точек, считаются выбросами.
\end{enumerate}

Выберем какой-нибудь корневой объект  из датасета, пометим его и поместим всех его непосредственно плотно-достижимых соседей в список обхода. Теперь для каждой  из списка: пометим эту точку, и, если она тоже корневая, добавим всех её соседей в список обхода. Тривиально доказывается, что кластеры помеченных точек, сформированные в ходе этого алгоритма максимальны (т.е. их нельзя расширить ещё одной точкой, чтобы удовлетворялись условия) и связны в смысле плотно-достижимости. Отсюда следует, что если мы обошли не все точки, можно перезапустить обход из какого-нибудь другого корневого объекта, и новый кластер не поглотит предыдущий.

\subsection{Параметры алгоритма \code{DBSCAN}}

DBSCAN требует задания двух параметров:

\begin{itemize}
	\item \code{eps} ($\epsilon$) -- размер окрестности, в рамках которой будут рассматриваться другие точки для определения принадлежности к кластеру текущей точки.
	\item \code{min_samples} ($m$) -- минимальное число вокруг точки, которые образуют с ней плотную область.
\end{itemize}

Соотношение $\alpha = \dfrac{m}{\epsilon^n}$, где $n$ -- размерность пространства, можно интуитивно рассматривать как пороговую плотность точек данных в области пространства. Ожидаемо, что при одинаковом соотношении $\alpha$, и результаты будут примерно одинаковы. Иногда это действительно так, но есть причина, почему алгоритму нужно задать два параметра, а не один. Во-первых типичное расстояние между точками в разных датасетах разное — явно задавать радиус приходится всегда. Во-вторых, играют роль неоднородности датасета. Чем больше $m$ и $\epsilon$, тем больше алгоритм склонен «прощать» вариации плотности в кластерах. С одной стороны, это может быть полезно: неприятно увидеть в кластере «дырки», где просто не хватило данных. С другой стороны, это вредно, когда между кластерами нет чёткой границы или шум создаёт «мост» между скоплениями. Тогда DBSCAN запросто соединит две разные группы. В балансе этих параметров и кроется сложность применения DBSCAN: реальные наборы данных содержат кластеры разной плотности с границами разной степени размытости \cite{habr}.

\subsection{Реализация алгоритма}

Реализуем алгоритм как наследника класса \code{Estimator} из библиотеки \code{scikit-learn} для языка Python. Это позволит использовать реализованный алгоритм в тех местах, где ожидается экземпляр класса \code{Estimator}, способный обучиться на основе данных при помощи метода \code{fit} и вернуть найденные кластеры в исходных данных.

\newpage

\begin{lstlisting}
class DBSCAN(BaseEstimator, ClusterMixin):

    def __init__(self, eps, min_samples):
        self.eps = eps
        self.min_samples = min_samples

    def fit(self, P, y=None):
        NOISE = 0
        C = 0
        N = P.shape[0]
        
        visited = np.zeros(N, dtype=bool)
        clustered = np.zeros(N, dtype=bool)
        labels = np.zeros(N, dtype=int)
        
        def region_query(i):
            return [j for j in np.arange(N) if np.linalg.norm(P[i] - P[j]) < self.eps]
        
        def expand_cluster(i, neighbours):
            p = P[i]
            labels[i] = C
            clustered[i] = True
            while neighbours:
                j = neighbours.pop()
                q = P[j]
                if not visited[j]:
                    visited[j] = True
                    neighbour_neighbours = region_query(j)
                    if len(neighbour_neighbours) >= self.min_samples:
                        neighbours.extend(neighbour_neighbours)
                clustered[j] = True
                labels[j] = C

        for i in np.arange(N):
            p = P[i]
            if visited[i]:
                continue
            visited[i] = True
            neighbours = region_query(i)
            if len(neighbours) < self.min_samples:
                labels[i] = NOISE
            else:
                C += 1
                expand_cluster(i, neighbours)

        self.labels_ = labels
        
        return self
\end{lstlisting}

В конструкторе алгоритм получает описанные выше параметры. В методе \code{fit} происходит кластеризация. Анализируется каждая еще не посещенная точка; если она является корневой, то алгоритм пытается расширить кластер. Результатом работы алгоритма является список присвоенных классов (\code{labels}) для входных данных.

\subsection{Результаты работы на тестовых примерах}

Проверку работоспособности реализованного алгоритма \code{DBSCAN} будем оценивать на тестовых примерах базы \cite{data}. Этот датасет включает в себя данные различной формы и распределения.

\subsubsection{Atom}

\data{Atom}{Разные дисперсии и линейно неразделимые кластеры}

Видно, что в первом случае параметры подобраны неверно, из-за чего найдено всего 3 кластера и очень большое число точек помечены как выбросы (черный цвет). При параметрах во втором случае результат совпадает с ожидаемым (2 кластера, нет выбросов).

\subsubsection{Chainlink}

\data{Chainlink}{Линейно не разделимые кластеры}

Видно, что при подобранных параметрах удалось безошибочно произвести кластеризацию.

\subsubsection{EngyTime}

\data{EngyTime}{Смесь Гауссовых распределений}

В данном случае кластеры находятся очень близко и подобрать оптимальные значения $\epsilon$ и $m$ не удалось.

\subsubsection{GolfBall}

\data{GolfBall}{Кластеры отсутствуют}

Алгоритм успешно определил, что выраженных кластеров нет и присвоил всем точкам один и тот же класс.

\newpage

\subsubsection{Hepta}

\data{Hepta}{Явно различимые кластеры, разные дисперсии}

В данном случае на очень широком диапазоне входных параметров алгоритм успешно определяет присутствие 7 кластеров, т.к. они явно различимы.

\subsubsection{Lsun}

\data{Lsun}{Разные дисперсии и расстояния между кластерами}

Видно, что при подобранных значениях $\epsilon = 0.5$ и $m = 10$ алгоритм безошибочно выделил кластеры.

\newpage

\subsubsection{Target}

\data{Target}{Присутствуют выбросы}

Видно, что в первый раз значение $m$ было слишком мало (3), из-за чего выбросы сформировали 4 дополнительных кластера. При $m = 10$ эти точки были помечены как шум (черный цвет).

\subsubsection{Tetra}

\data{Tetra}{Близкие кластеры}

В данном случае кластеры близки, но различимы для алгоритма, поэтому кластеризация прошла без ошибок.

\newpage

\subsubsection{TwoDiamonds}

\data{TwoDiamonds}{Границы кластера определяются плотностью}

Видно, что кластеры различимы. Единственная ошибка алгоритма -- центральная точка (зависит от параметров).

\subsubsection{WingNut}

\data{WingNut}{Плотность vs. расстояние}

Алгоритм \code{DBSCAN} основывается на плотности, и, в данном случае, расстояния между точками одного кластера слишком велико. Из-за этого не удалось подобрать подходящие парамеры $\epsilon$ и $m$.

\newpage

\subsection{Достоинства, недостатки и улучшения}

\noindent Достоинства \code{DBSCAN}:

\begin{itemize}
	\item Не требует спецификации числа кластеров.
	\item Форма кластеров может быть произвольной.
	\item Имеет устойчивость к шуму и выбросам.
	\item Большей частью нечувствителен к порядку точек в базе данных.
\end{itemize}

\noindent Недостатки \code{DBSCAN}:

\begin{itemize}
	\item Подбор параметров $\epsilon$ и $m$ зависит от исходных данных (существуют эвристики для выбора параметров).
	\item Не полностью однозначен -- краевые точки, которые могут быть достигнуты из более чем одного кластера, могут принадлежать любому из этих кластеров, что зависит от порядка просмотра точек.
	\item Не может хорошо кластеризовать наборы данных с большой разницей в плотности, поскольку не удается выбрать приемлемые параметры для всех кластеров.
	\item Алгоритм не способен соединять кластеры через проемы, и, наоброт, способен соединить явно различные кластеры через проемы с высокой плотностью точек.
\end{itemize}

\noindent Улучшения и расширения \code{DBSCAN}:

\begin{itemize}
	\item Алгоритм может быть распараллелен.
	\item Множества $E(x)$ можно не пересчитывать каждый раз, а использовать дополнительные структуры данных -- KD-tree или др.
	\item Алгоритм \code{GDBSCAN} -- обобщение \code{DBSCAN} для произвольных логических выражений «соседства» и «плотности» \cite{gdbscan}.
	\item Гибридный алгоритм \code{FastDBSCAN} на основе \code{DBSCAN} и \code{K-Means} \cite{fastdbscan}.
\end{itemize}

\subsection{Альтернативные алгоритмы}

Кластеризацией данных занимается раздел статистики под названием кластерный анализ. В рамках кластерного анализа общепринятой классификации методов кластеризации не существует, но можно выделить ряд групп подходов \cite{wiki-clustering}:

\begin{enumerate}
	\item Вероятностный подход. Предполагается, что каждый рассматриваемый объект относится к одному из $k$ классов.
	\begin{itemize}
		\item K-средних
		\item К-медиан
		\item EM-алгоритм
		\item Алгоритмы семейства FOREL
		\item Дискриминантный анализ
	\end{itemize}
	\item Подходы на основе систем искусственного интеллекта: весьма условная группа, так как методов много и методически они весьма различны.
	\begin{itemize}
		\item Метод нечеткой кластеризации C-средних (C-means)
		\item Нейронная сеть Кохонена
		\item Генетический алгоритм
	\end{itemize}
	\item Логический подход. Построение дендрограммы осуществляется с помощью дерева решений.
	\item Теоретико-графовый подход.
	\begin{itemize}
		\item Графовые алгоритмы кластеризации
	\end{itemize}
	\item Иерархический подход. Предполагается наличие вложенных групп (кластеров различного порядка). Алгоритмы в свою очередь подразделяются на агломеративные (объединительные) и дивизивные (разделяющие). По количеству признаков иногда выделяют монотетические и политетические методы классификации.
	Иерархическая дивизивная кластеризация или таксономия. Задачи кластеризации рассматриваются в количественной таксономии.
	\item Другие методы. Не вошедшие в предыдущие группы.
	\begin{itemize}
		\item Статистические алгоритмы кластеризации
		\item Ансамбль кластеризаторов
		\item Алгоритмы семейства KRAB
		\item Алгоритм, основанный на методе просеивания
	\end{itemize}
\end{enumerate}

\section{Выводы}

В рамках данной работы:

\begin{itemize}
	\item изучен алгоритм кластеризации данных \code{DBSCAN}; \item рассмотрены параметры алгоритма и их влияние на его работу;
	\item реализован алгоритм \code{DBSCAN} на языке Python\footnote{\url{https://github.com/vaddya/intelligent-systems/blob/master/lab8/lab8.ipynb}};
	\item проверена работоспособность алгоритма на 10 различных тестовых примерах; на большей их части алгоритм верно кластеризовал данные;
	\item описаны достоинства и недостатки алгоритма, а также рассмотрены существующие расширения алгоритма;
	\item изучены альтернативные алгоритмы для решения задачи кластеризации.
\end{itemize}

\newpage

\bibliographystyle{plain}
\addcontentsline{toc}{section}{Список литературы}
\bibliography{refs}

\end{document}
